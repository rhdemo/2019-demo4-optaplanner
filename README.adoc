= Red Hat Summit OptaPlanner

== Build & run using Spring Boot

`mvn spring-boot:run`

To run with the Infinispan connector:

1. Run infinispan server (make sure counters and DispatchEvents caches are added to the config file):

    $INFINISPAN_HOME/bin/standalone.sh -c CONFIG_FILE.xml

2. Change hotrod endpoint in `hotrod-client.properties`:

    infinispan.client.hotrod.endpoint=localhost

3. Run the application:

    `mvn spring-boot:run -Dprod`

== Build & run on OpenShift

1. Login to a running OpenShift instance

    oc login -u USERNAME -p PASSWORD OPENSHIFT_URL

2. Make sure datagrid service is running, if not deploy it:

    git clone git@github.com:rhdemo/2019-demo4-cluster-setup.git
    ./2019-demo4-cluster-setup/datagrid/deploy.sh

3. Either use an existing project or create a new one by executing:

    oc new-project NAME

3. Deploy the application to the project

    mvn clean package -Popenshift

4. Find out a URL the service was exposed on

    oc get route

== Infinispan relevant data

Below are examples on key/value pairs in Infinispan caches that are relevant to OptaPlanner.

=== Cache `DispatchMechanic`


```
MECHANIC_ID: {
    responseType: "DISPATCH_MECHANIC" or "ADD_MECHANIC",
    mechanic: {
        "mechanicIndex": 0,
        “originalMachineIndex”: 0,
        "focusMachineIndex": 0,
        "focusTravelDurationMillis": 1000,
        "focusFixDurationMillis": 2000
    }
}
```

```
MECHANIC_ID-futureIndexes: {
    responseType: "UPDATE_FUTURE_VISITS",
    mechanicIndex: 0,
    futureMachineIndexes: [
        0,
        1,
        ...
    ]
}
```

=== Cache `game`
```
OptaPlannerConfig: {
    dispatchActive: true/false,
    simulationActive: true/false
}
```

== Building an image and pushing it to a docker registry

First make sure docker daemon is running, if not type `sudo systemctl start docker`
(or add the property `-Ddocker.host=DOCKER_DAEMON_URL`).

Create a `.env` file similar to `.env.example` containing your quay.io username and password and run the script

```
./pushImage.sh
```

This command will build an image `optaplanner-demo:latest` and tag it temporarily as
`quay.io/redhatdemo/optaplanner-demo:latest` then push it to quay.io registry.
Therefore, remove any images with the same tag before executing the command above.

=== Troubleshooting

Running the `docker` command without `sudo` should work, but doesn't on a vanilla Fedora installation.
If it doesn't, try this:

```
dnf install docker
sudo groupadd docker
sudo usermod -aG docker `whoami`
sudo systemctl start docker
// If this fails with a permission problem, reboot first
docker run hello-world
```

If you're getting 401 authorization issues during the `mvn` build, try this first:

```
docker login docker.io
docker login quay.io/redhatdemo
```

If you follow security protocols and have different passwords for both services,
run `docker pull fabric8/java-centos-openjdk8-jdk:1.5` first with the docker.io credentials
and then run the `mvn` command with the quay credentials.


=== Running Mechanical Scheduling demo only (optaplanner, infinispan, webgame server & dashboard UI)

1. Clone cluster-setup repostiory:

  git clone git@github.com:rhdemo/2019-demo4-cluster-setup.git

2. Copy the environment variables file:

  cp .env.example .env

3. Change the first four OC* variables to match your oc cluster info
4. In .env under # Frontend, add `FRONTEND_DEV=true` and change `ADMIN_PASSWORD` which will be required
to access the admin UI.
5. Change the number of datagrid pods from 10 to 1 by changing the value of `INSTANCES` variable in `datagrid/deploy.sh`
6. Remove all resources requests from projects files so the demo can run on a cluster with limited resources.
These files are:
** `optaplanner/optaplanner-demo-template.yaml`
** `datagrid/datagrid-service-custom-xml.yaml`
** `frontend/*.yml`

For example, in `optaplanner/optaplanner-demo-template.yaml` line 60 specifies the cpu and memory requests as 4 cpu cores and 4GB of RAM.

7. Now you will be able to deploy the demo apps, run:

  make datagrid
  make optaplanner
  make frontend

8. Wait until the pods are running, then you will be able to access the admin UI at https://admin-web-game-demo.127.0.0.1.nip.io
using the password that you set in step 4 with the variable `ADMIN_PASSWORD`.
